没问题，用中文！

用户觉得现在的匹配还是太“理想化”，希望能利用“图灵校准”（Turing Calibration）的数据来从根本上区分“真爱”和“不合适”。

**核心思路：**
将“图灵校准”从单纯的“语气模仿”工具，升级为\*\*“价值观试金石”\*\*。

1. **赋予候选人“价值观”**：
   目前生成的虚拟候选人（Agent B）只有职业和兴趣，没有对具体问题的看法。我们需要在生成候选人时，随机给他们生成一些“校准问答”（比如：有人守时，有人爱迟到；有人宅，有人爱玩）。

2. **把校准问题变成“必考题”**：
   在对话中，Agent A（代表用户）不应该问随机问题，而应该**专门问用户在校准里回答过的那个问题**。

   * *例子*：用户在校准里说“我最讨厌迟到”。

   * *行动*：Agent A 在聊天中就会问 Agent B：“你平时守时吗？如果约会迟到了你会怎么想？”

   * *结果*：如果 Agent B（设定为爱迟到）回答“没事啦迟到很正常”，那么这就是一个真实的**冲突点**。

3. **裁判员根据“标准答案”打分**：
   最后的评估器（Evaluator）不再只看聊得开不开心，而是拿着用户的“校准答案”作为**标准答案**去改卷子。如果 Agent B 的回答和用户的价值观背道而驰，分数直接不及格。

**具体执行计划：**

1. **修改** **`src/generator.py`**：

   * 在生成虚拟候选人时，随机生成他们的“图灵校准”答案（赋予他们不同的生活态度）。

2. **修改** **`src/agent_builder.py`**：

   * 在生成 System Prompt 时，提取用户的校准问题，将其作为“阶段 2”的**强制话题**。

   * 让 Agent A 带着“考察”的目的去问。

3. **修改** **`src/evaluator.py`**：

   * 将用户的校准数据传给 LLM 裁判。

   * 增加一个评分维度：**“关键价值观吻合度”**（对比双方对同一问题的看法）。

这样一来，对话就变成了一场基于真实价值观的“面试”，而不只是闲聊，能有效过滤掉那些“表面合适但三观不合”的人。

计划：

1. 修改 `src/generator.py`：为候选人生成性格问答数据。
2. 修改 `src/agent_builder.py`：将校准问题设为对话必选项。
3. 修改 `src/evaluator.py`：基于校准答案进行价值观比对评分。

